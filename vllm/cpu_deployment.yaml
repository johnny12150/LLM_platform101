apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm
  namespace: llm-platform
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vllm
  template:
    metadata:
      labels:
        app: vllm
    spec:
      initContainers:
        - name: download-model
          image: alpine:3.23.2
          env:
            - name: MINIO_ENDPOINT
              value: "http://llm-minio-hl.llm-platform.svc.cluster.local:9000"
            - name: MINIO_BUCKET
              value: "my-bucket"
            - name: MINIO_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: llm-minio-user
                  key: CONSOLE_ACCESS_KEY
            - name: MINIO_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: llm-minio-user
                  key: CONSOLE_SECRET_KEY
          command: ["/bin/sh", "-c"]
          args:
            - |
              set -e
              apk add --no-cache tar curl
              curl https://dl.min.io/client/mc/release/linux-amd64/mc \
              --create-dirs -o /usr/local/bin/mc && \
              chmod +x /usr/local/bin/mc
              
              mc alias set minio ${MINIO_ENDPOINT} ${MINIO_ACCESS_KEY} ${MINIO_SECRET_KEY}
              mc cp \
                minio/${MINIO_BUCKET}/deepseek-r1-distill-qwen-1b.tar.gz \
                /models
              tar -xzf /models/deepseek-r1-distill-qwen-1b.tar.gz -C /models
              rm -f /models/deepseek-r1-distill-qwen-1b.tar.gz
          volumeMounts:
            - name: model-cache
              mountPath: /models
      containers:
        - name: vllm
          image: vllm/vllm-openai:v0.6.4.post1
          imagePullPolicy: IfNotPresent
          env:
            - name: HF_HOME
              value: /models
#            - name: TRANSFORMERS_OFFLINE
#              value: "1"
#            - name: HF_HUB_OFFLINE
#              value: "1"
#            - name: VLLM_LOGGING_LEVEL
#              value: "DEBUG"
          args:
            - "--model"
            - "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
            - "--device"
            - "cpu"
            - "--max-model-len"
            - "2048"
            - "--disable-async-output-proc"
#            - "--worker-cls"
#            - "vllm.worker.cpu_worker.CPUWorker"
          ports:
            - containerPort: 8000
          resources:
            limits:
              cpu: "6"
              memory: "14Gi"
            requests:
              cpu: "4"
              memory: "12Gi"
          volumeMounts:
            - name: model-cache
              mountPath: /models
      volumes:
        - name: model-cache
          emptyDir: {}